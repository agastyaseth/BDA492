{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UV Decomposition Example\n",
    "## Assignment 2: Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The simplest form of matrix decomposition is to find a pair of matrixes, the first (p) with few columns and the second (q) with few rows, whose product is close to the given matrix M.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The axes of the subspace can be chosen by:\n",
    "* The first dimension is the direction in which the points exhibit the greatest variance.\n",
    "* The second dimension is the direction, orthogonal to the first, in which points show the greatest variance.\n",
    "* And so on…, until you have “enough” dimensions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Error (RSME)\n",
    "* Common way to evaluate how well P = UV approximates M is by RMSE (root-mean-square error).\n",
    "* Average $(m_{ij} – p_{ij})^2$ over all i and j.\n",
    "* Take the square root.\n",
    "* Square-rooting changes the scale of error, but doesn’t really effect which choice of U and V is best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "* Pick r, the number of latent factors.\n",
    "* Think of U and V as composed of variables, uik and vkj.\n",
    "* Express the RMSE as (the square root of): $$e =  \\sum_{ij} (r_{ij} – \\sum_k p_{ik} q_{kj})^2 $$\n",
    "* Gradient descent: repeatedly find the derivative of E with respect to each variable and move each a small amount in the direction that lowers the value of E.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "The above algorithm is a very basic algorithm for factorizing a matrix. There are a lot of methods to make things look more complicated. A common extension to this basic algorithm is to introduce regularization to avoid overfitting. This is done by adding a parameter $\\beta$ and modify the squared error as follows:\n",
    "\n",
    "$$e_{ij}^2 = (r_{ij}- \\sum_{k=1}^K p_{ik}q_{kj})^2 + \\beta/2 \\sum_{k=1}^K (||P||^2 + ||Q||^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run\n",
    "\n",
    "Running the program is really straightforward:\n",
    "\n",
    "* Open the .ipynb file in Jupyter Notebook\n",
    "* Run all the cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UV Decomposition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uvd(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02): # alpha = learning rate; beta = regularizing parameter\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i,j] > 0:\n",
    "                    eij = R[i,j] - np.dot(P[i,:],Q[:,j])\n",
    "                    for k in range(K):\n",
    "                        P[i,k] = P[i,k] + alpha * (2 * eij * Q[k,j] - beta * P[i,k])\n",
    "                        Q[k,j] = Q[k,j] + alpha * (2 * eij * P[i,k] - beta * Q[k,j])\n",
    "        eR = np.dot(P,Q)\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i,j] > 0:\n",
    "                    e = e + pow(R[i,j] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * (pow(P[i,k],2) + pow(Q[k,j],2))\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working out an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 0, 1],\n",
       "       [4, 0, 0, 1],\n",
       "       [1, 1, 0, 5],\n",
       "       [1, 0, 0, 4],\n",
       "       [0, 1, 5, 4]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = [[5,3,0,1],[4,0,0,1],[1,1,0,5],[1,0,0,4],[0,1,5,4],] #A sparse matrix with missing values as '0'\n",
    "R = np.array(R)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.97727843, 2.98467942, 0.8529939 , 1.00006764],\n",
       "       [3.98192378, 2.40638611, 0.92702492, 0.99732324],\n",
       "       [0.98727648, 1.03903922, 6.05359292, 4.94338625],\n",
       "       [1.00793613, 0.95913662, 4.84221357, 3.96785905],\n",
       "       [0.9862434 , 0.95471   , 4.95146456, 4.05459523]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(R)\n",
    "M = len(R[0])\n",
    "K = 2\n",
    "\n",
    "#assigning random matrices for P and Q\n",
    "P = numpy.random.rand(N,K) \n",
    "Q = numpy.random.rand(M,K)\n",
    "\n",
    "nP, nQ = uvd(R, P, Q, K)\n",
    "nR = np.dot(nP, nQ.T)\n",
    "nR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that we have a matrix that closely resembles the original matrix R, with a good approximation of the missing values.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
